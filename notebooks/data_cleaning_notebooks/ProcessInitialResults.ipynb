{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Initial Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from data_generation_scripts.utils import check_rate_limit, check_add_orgs, check_add_repos, check_add_users\n",
    "from data_generation_scripts.generate_expanded_search_data import get_initial_search_datasets\n",
    "from data_generation_scripts.generate_repo_metadata import get_repo_languages, get_repo_labels, get_repo_tags,  get_repo_profile, get_total_commits\n",
    "from data_generation_scripts.generate_repo_users_interactions import get_repos_user_actors\n",
    "from data_generation_scripts.generate_repo_metadata import check_total_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've run `generate_expanded_search_data.py` and then `check_clean_search_results.py` you'll have a series of files in the `data/` directory that contain the results of your search. This notebook will help you process those results into a single file that can be used for analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of how to run `generate_expanded_search_data.py`:\n",
    "\n",
    "```python3\n",
    "rates_df = check_rate_limit()\n",
    "initial_repo_output_path = \"../data/repo_data/\"\n",
    "repo_output_path = \"../data/large_files/entity_files/repos_dataset.csv\"\n",
    "repo_join_output_path = \"../data/large_files/join_files/search_queries_repo_join_dataset.csv\"\n",
    "\n",
    "initial_user_output_path = \"../data/user_data/\"\n",
    "user_output_path = \"../data/entity_files/users_dataset.csv\"\n",
    "user_join_output_path = \"../data/join_files/search_queries_user_join_dataset.csv\"\n",
    "load_existing_data = False\n",
    "overwrite_existing_temp_files = False\n",
    "org_output_path = \"../data/entity_files/orgs_dataset.csv\"\n",
    "\n",
    "get_initial_search_datasets(rates_df, initial_repo_output_path,  repo_output_path, repo_join_output_path, initial_user_output_path, user_output_path, user_join_output_path, org_output_path, overwrite_existing_temp_files, load_existing_data)\n",
    "```\n",
    "\n",
    "And then just run `check_clean_search_results.py` "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Initial Core Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = pd.read_csv(\"../../data/entity_files/users_dataset.csv\")\n",
    "repo_df = pd.read_csv(\"../../data/large_files/entity_files/repos_dataset.csv\", low_memory=False)\n",
    "org_df = pd.read_csv(\"../../data/entity_files/orgs_dataset.csv\", low_memory=False)\n",
    "search_queries_repo_join_df = pd.read_csv(\"../../data/derived_files/updated_search_queries_repo_join_subset_dh_dataset.csv\")\n",
    "search_queries_user_join_df = pd.read_csv(\n",
    "    \"../../data/derived_files/updated_search_queries_user_join_subset_dh_dataset.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial core datasets will be comprised of the following:\n",
    "\n",
    "- `core_repos`: A list of all repos that were returned by the search query\n",
    "- `core_users`: A list of all users that were returned by the search query\n",
    "- `core_orgs`: A list of all orgs that were returned by the search query"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if all items exist in entity files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 0, 0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_repos = search_queries_repo_join_df[~search_queries_repo_join_df.full_name.isin(repo_df.full_name)]\n",
    "missing_users = search_queries_user_join_df[(~search_queries_user_join_df.login.isin(user_df.login)) & (search_queries_user_join_df['type'] == 'User')]\n",
    "missing_orgs = search_queries_user_join_df[(~search_queries_user_join_df.login.isin(user_df.login)) & (search_queries_user_join_df['type'] == 'Organization')]\n",
    "\n",
    "len(missing_repos), len(missing_users), len(missing_orgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/metadata_files/repo_headers.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_repos) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     repo_df \u001b[39m=\u001b[39m check_add_repos(missing_repos, \u001b[39m'\u001b[39;49m\u001b[39m../../data/large_files/entity_files/repos_dataset.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      3\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_orgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m      4\u001b[0m     org_df \u001b[39m=\u001b[39m check_add_orgs(missing_orgs, \u001b[39m'\u001b[39m\u001b[39m../../data/entity_files/orgs_dataset.csv\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mTrue\u001b[39;00m, \u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/VersionsAndValues/notebooks/data_cleaning_notebooks/../../data_generation_scripts/utils.py:518\u001b[0m, in \u001b[0;36mcheck_add_repos\u001b[0;34m(potential_new_repo_df, repo_output_path, return_df)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[39m\"\"\"Function to check if repo are already in the repo file and add them if not \u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[39m:param potential_new_repo_df: dataframe of contributors\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[39m:param repo_output_path: path to repo file\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[39m:param return_df: boolean to return dataframe or not\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[39m:return: repo dataframe\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    517\u001b[0m error_file_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m../data/error_logs/potential_repos_errors.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 518\u001b[0m repo_headers \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39m../data/metadata_files/repo_headers.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    519\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(repo_output_path):\n\u001b[1;32m    520\u001b[0m     repo_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(repo_output_path)\n",
      "File \u001b[0;32m~/.virtualenvs/values_and_versions_env/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.virtualenvs/values_and_versions_env/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.virtualenvs/values_and_versions_env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/.virtualenvs/values_and_versions_env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.virtualenvs/values_and_versions_env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/.virtualenvs/values_and_versions_env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.virtualenvs/values_and_versions_env/lib/python3.9/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/metadata_files/repo_headers.csv'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if len(missing_repos) > 0:\n",
    "    repo_df = check_add_repos(missing_repos, '../../data/large_files/entity_files/repos_dataset.csv', True)\n",
    "if len(missing_orgs) > 0:\n",
    "    org_df = check_add_orgs(missing_orgs, '../../data/entity_files/orgs_dataset.csv', True, False)\n",
    "if len(missing_users) > 0:\n",
    "    user_df = check_add_users(missing_users, '../../data/entity_files/users_dataset.csv', True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2264, 2549, 2279)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_repos = repo_df[repo_df.full_name.isin(search_queries_repo_join_df.full_name.unique())]\n",
    "len(core_repos), len(search_queries_repo_join_df), search_queries_repo_join_df.full_name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "342"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_queries_repo_join_df.keep_resource.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>natural_language</th>\n",
       "      <th>finalized_language</th>\n",
       "      <th>keep_resource</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2149</th>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150</th>\n",
       "      <td>en, ny, ha, ig, lb, mg, sm, sn, st, tl, yo</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2152</th>\n",
       "      <td>en, ny, ha, ig, lb, mg, sm, sn, st, tl, yo</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2154</th>\n",
       "      <td>en, ny, ha, ig, lb, mg, sm, sn, st, tl, yo</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2155</th>\n",
       "      <td>en, ny, ha, ig, lb, mg, sm, sn, st, tl, yo</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2544</th>\n",
       "      <td>ko</td>\n",
       "      <td>ko</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2545</th>\n",
       "      <td>ko</td>\n",
       "      <td>ko</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2546</th>\n",
       "      <td>en, ny, ha, ig, lb, mg, sm, sn, st, tl, yo</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2547</th>\n",
       "      <td>en, ny, ha, ig, lb, mg, sm, sn, st, tl, yo</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2548</th>\n",
       "      <td>en, ny, ha, ig, lb, mg, sm, sn, st, tl, yo</td>\n",
       "      <td>zh-CN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>363 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                natural_language finalized_language  \\\n",
       "2149                                          en                 en   \n",
       "2150  en, ny, ha, ig, lb, mg, sm, sn, st, tl, yo                 en   \n",
       "2152  en, ny, ha, ig, lb, mg, sm, sn, st, tl, yo                 en   \n",
       "2154  en, ny, ha, ig, lb, mg, sm, sn, st, tl, yo                 en   \n",
       "2155  en, ny, ha, ig, lb, mg, sm, sn, st, tl, yo                 en   \n",
       "...                                          ...                ...   \n",
       "2544                                          ko                 ko   \n",
       "2545                                          ko                 ko   \n",
       "2546  en, ny, ha, ig, lb, mg, sm, sn, st, tl, yo                 en   \n",
       "2547  en, ny, ha, ig, lb, mg, sm, sn, st, tl, yo                 en   \n",
       "2548  en, ny, ha, ig, lb, mg, sm, sn, st, tl, yo              zh-CN   \n",
       "\n",
       "     keep_resource  \n",
       "2149           NaN  \n",
       "2150           NaN  \n",
       "2152           NaN  \n",
       "2154           NaN  \n",
       "2155           NaN  \n",
       "...            ...  \n",
       "2544           NaN  \n",
       "2545           NaN  \n",
       "2546           NaN  \n",
       "2547           NaN  \n",
       "2548           NaN  \n",
       "\n",
       "[363 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_queries_repo_join_df[(search_queries_repo_join_df.full_name.isin(core_repos.full_name.unique())) & (search_queries_repo_join_df.keep_resource != True)][['natural_language', 'finalized_language', 'keep_resource']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_terms = [\"Digital Humanities\"]\n",
    "# console = Console()\n",
    "initial_repo_output_path = \"../../data/repo_data/\"\n",
    "repo_output_path = \"../../data/large_files/entity_files/repos_dataset.csv\"\n",
    "initial_repo_join_output_path = \"../../data/large_files/join_files/search_queries_repo_join_dataset.csv\"\n",
    "existing_search_queries_repo_file_path = \"../../data/derived_files/updated_search_queries_repo_join_subset_dh_dataset.csv\"\n",
    "\n",
    "initial_user_output_path = \"../../data/user_data/\"\n",
    "user_output_path = \"../../data/entity_files/users_dataset.csv\"\n",
    "org_output_path = \"../../data/entity_files/orgs_dataset.csv\"\n",
    "initial_user_join_output_path = \"../../data/join_files/search_queries_user_join_dataset.csv\"\n",
    "existing_search_queries_user_file_path = \"../../data/derived_files/updated_search_queries_user_join_subset_dh_dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_join_output_path = \"search_queries_repo_join_dataset.csv\"\n",
    "user_join_output_path = \"search_queries_user_join_dataset.csv\"\n",
    "join_unique_field = 'search_query'\n",
    "filter_fields = ['id', 'cleaned_search_query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_search_queries_user_df = pd.read_csv(existing_search_queries_user_file_path)\n",
    "existing_search_queries_repo_df = pd.read_csv(existing_search_queries_repo_file_path)\n",
    "\n",
    "existing_search_queries_user_df = existing_search_queries_user_df[existing_search_queries_user_df.search_term_source.isin(subset_terms)]\n",
    "existing_search_queries_repo_df = existing_search_queries_repo_df[existing_search_queries_repo_df.search_term_source.isin(subset_terms)]\n",
    "existing_search_queries_user_df['cleaned_search_query'] = existing_search_queries_user_df['search_query'].str.replace('%22', '').str.replace('\"', '').str.replace('%3A', ':').str.split('&page').str[0]\n",
    "existing_search_queries_repo_df['cleaned_search_query'] = existing_search_queries_repo_df['search_query'].str.replace('%22', '\"').str.replace('\"', '').str.replace('%3A', ':').str.split('&page').str[0]\n",
    "\n",
    "updated_search_queries_repo_df = check_for_joins_in_older_queries(repo_join_output_path, existing_search_queries_repo_df, join_unique_field, filter_fields)\n",
    "updated_search_queries_user_df = check_for_joins_in_older_queries(user_join_output_path, existing_search_queries_user_df, join_unique_field, filter_fields)\n",
    "# updated_search_queries_repo_df = updated_search_queries_repo_df.drop_duplicates(subset=['id', 'cleaned_search_query'])\n",
    "# updated_search_queries_user_df = updated_search_queries_user_df.drop_duplicates(subset=['id', 'cleaned_search_query'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "values_and_versions_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
