{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Initial Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from data_generation_scripts.general_utils import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've run `generate_expanded_search_data.py` and then `check_clean_search_results.py` you'll have a series of files in the `data/` directory that contain the results of your search. This notebook will help you process those results into a single file that can be used for analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of how to run `generate_expanded_search_data.py`:\n",
    "\n",
    "```python3\n",
    "rates_df = check_rate_limit()\n",
    "initial_repo_output_path = \"../data/repo_data/\"\n",
    "repo_output_path = \"../data/large_files/entity_files/repos_dataset.csv\"\n",
    "repo_join_output_path = \"../data/large_files/join_files/search_queries_repo_join_dataset.csv\"\n",
    "\n",
    "initial_user_output_path = \"../data/user_data/\"\n",
    "user_output_path = \"../data/entity_files/users_dataset.csv\"\n",
    "user_join_output_path = \"../data/join_files/search_queries_user_join_dataset.csv\"\n",
    "load_existing_data = False\n",
    "overwrite_existing_temp_files = False\n",
    "org_output_path = \"../data/entity_files/orgs_dataset.csv\"\n",
    "\n",
    "get_initial_search_datasets(rates_df, initial_repo_output_path,  repo_output_path, repo_join_output_path, initial_user_output_path, user_output_path, user_join_output_path, org_output_path, overwrite_existing_temp_files, load_existing_data)\n",
    "```\n",
    "\n",
    "And then just run `check_clean_search_results.py` "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Initial Core Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory_path = \"../../datasets\"\n",
    "user_df = read_csv_file(f\"{data_directory_path}/large_files/entity_files/users_dataset.csv\")\n",
    "repo_df = read_csv_file(f\"{data_directory_path}/large_files/entity_files/repos_dataset.csv\")\n",
    "org_df = read_csv_file(f\"{data_directory_path}/entity_files/orgs_dataset.csv\")\n",
    "search_queries_repo_join_df = read_csv_file(f\"{data_directory_path}/derived_files/initial_search_queries_repo_join_subset_dh_dataset.csv\")\n",
    "search_queries_user_join_df = read_csv_file(f\"{data_directory_path}/derived_files/initial_search_queries_user_join_subset_dh_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_join_output_path = f\"{data_directory_path}/derived_files/initial_search_queries_repo_join_subset_dh_dataset.csv\"\n",
    "user_join_output_path = f\"{data_directory_path}/derived_files/initial_search_queries_user_join_subset_dh_dataset.csv\"\n",
    "\n",
    "search_queries_repo_df = read_csv_file(repo_join_output_path)\n",
    "search_queries_user_df = read_csv_file(user_join_output_path)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial core datasets will be comprised of the following:\n",
    "\n",
    "- `core_repos`: A list of all repos that were returned by the search query\n",
    "- `core_users`: A list of all users that were returned by the search query\n",
    "- `core_orgs`: A list of all orgs that were returned by the search query"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if all items exist in entity files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 3, 12)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_repos = search_queries_repo_join_df[~search_queries_repo_join_df.full_name.isin(repo_df.full_name)]\n",
    "missing_users = search_queries_user_join_df[(~search_queries_user_join_df.login.isin(user_df.login)) & (search_queries_user_join_df['type'] == 'User')]\n",
    "missing_orgs = search_queries_user_join_df[(~search_queries_user_join_df.login.isin(user_df.login)) & (search_queries_user_join_df['type'] == 'Organization')]\n",
    "\n",
    "len(missing_repos), len(missing_users), len(missing_orgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(missing_repos) > 0:\n",
    "    repo_df = check_add_new_entities(missing_repos, f'{data_directory_path}/large_files/entity_files/repos_dataset.csv', 'repos', True, False)\n",
    "if len(missing_orgs) > 0:\n",
    "    org_df = check_add_new_entities(missing_orgs, f'{data_directory_path}/entity_files/orgs_dataset.csv', 'orgs', True, False)\n",
    "if len(missing_users) > 0:\n",
    "    user_df = check_add_new_entities(missing_users, f'{data_directory_path}/entity_files/users_dataset.csv', 'users', True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_repos = pd.merge(repo_df, search_queries_repo_join_df[['full_name', 'finalized_language', 'keep_resource']], on='full_name', how='inner')\n",
    "core_repos = core_repos.drop_duplicates(subset=['full_name'])\n",
    "core_users = pd.merge(user_df, search_queries_user_join_df[['login', 'finalized_language', 'keep_resource']], on='login', how='inner')\n",
    "core_users = core_users.drop_duplicates(subset=['login'])\n",
    "core_orgs = core_users[core_users['type'] == 'Organization']\n",
    "core_users = core_users[core_users['type'] == 'User']\n",
    "\n",
    "len(core_repos), len(core_users), len(core_orgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_users_path = f\"{data_directory_path}/derived_files/core_users_dataset.csv\"\n",
    "core_repos_path = f\"{data_directory_path}/derived_files/core_repos_dataset.csv\"\n",
    "core_orgs_path = f\"{data_directory_path}/derived_files/core_orgs_dataset.csv\"\n",
    "\n",
    "if os.path.exists(core_users_path):\n",
    "    existing_core_users = pd.read_csv(core_users_path)\n",
    "\n",
    "    missing_cols = [col for col in existing_core_users.columns if col not in core_users.columns]\n",
    "    if len(missing_cols) > 0:\n",
    "        missing_cols = missing_cols + ['login']\n",
    "        added_cols = existing_core_users[missing_cols]\n",
    "        core_users = pd.merge(core_users, added_cols, on='login', how='left')\n",
    "        core_users = core_users.drop_duplicates(subset=['login'])\n",
    "\n",
    "    if len(core_users) > len(existing_core_users):\n",
    "        updated_core_users = core_users[~core_users.login.isin(existing_core_users.login)]\n",
    "        core_users = pd.concat([existing_core_users, updated_core_users])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite_files = False\n",
    "if os.path.exists(core_repos_path) == False or overwrite_files:\n",
    "    core_repos.to_csv(core_repos_path, index=False)\n",
    "if os.path.exists(core_users_path) == False or overwrite_files:\n",
    "    core_users.to_csv(core_users_path, index=False)\n",
    "if os.path.exists(core_orgs_path) == False or overwrite_files:\n",
    "    core_orgs.to_csv(core_orgs_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "values_and_versions_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
