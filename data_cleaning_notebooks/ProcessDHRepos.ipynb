{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process DH Repos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will be run multiple times as we add new repositories to the DH repo list. It will take the data from the DH repo list and process it through making additional calls to GitHub's APIs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries and Set Initial Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from data_generation_scripts.utils import check_rate_limit, check_return_error_file, read_combine_files, check_total_pages, check_for_joins_in_older_queries, check_add_users, combined_updated_users, check_for_entity_in_older_queries\n",
    "from data_generation_scripts.generate_expanded_search_data import get_initial_search_datasets\n",
    "from data_generation_scripts.generate_repo_metadata import get_repo_languages, get_repo_labels, get_repo_tags,  get_repo_profile, get_total_commits, check_total_results\n",
    "from data_generation_scripts.generate_repo_users_interactions import get_repos_user_actors\n",
    "from data_generation_scripts.general_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates_df = check_rate_limit()\n",
    "datafile_path = \"../../datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_core_repos = read_csv_file(f\"{data_directory_path}/derived_files/initial_core_repos.csv\")\n",
    "core_repos = read_csv_file(f\"{data_directory_path}/derived_files/firstpass_core_repos.csv\")\n",
    "len(initial_core_repos), len(core_repos)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Missing Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_fields = read_csv_file(f'{data_directory_path}/metadata_files/repo_url_cols.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_fields.loc[counts_fields.url_type == 'review_comments_url', 'count_type'] = 'review_count'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(repo_df, url_type, count_type, overwrite_existing_temp_files = False):\n",
    "    if count_type in repo_df.columns:\n",
    "        needs_counts = repo_df[repo_df[count_type].isna()]\n",
    "        has_counts = repo_df[repo_df[count_type].notna()]\n",
    "    else:\n",
    "        needs_counts = repo_df\n",
    "        has_counts = pd.DataFrame()\n",
    "        \n",
    "    if len(has_counts) == len(repo_df):\n",
    "        repo_df = has_counts\n",
    "    else:\n",
    "        needs_counts = check_total_results(needs_counts, url_type, overwrite_existing_temp_files)\n",
    "        repo_df = pd.concat([needs_counts, has_counts])\n",
    "    return repo_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting pulls_count for pulls_url\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "skip_types = ['review_comments_url', 'commits_url', 'collaborators_url']\n",
    "overwrite_existing_temp_files = True\n",
    "for index, row in counts_fields.iterrows():\n",
    "    if (row.url_type not in skip_types):\n",
    "        count_type = row.url_type.split(\"_\")[0] + \"_count\"\n",
    "        print(f\"Getting {count_type} for {row['url_type']}\")\n",
    "        if (count_type not in core_repos.columns) or (core_repos[count_type].isna().any()):\n",
    "            core_repos = get_counts(core_repos, row['url_type'], count_type, overwrite_existing_temp_files)\n",
    "        row['count_type'] = count_type\n",
    "        clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core_repos.to_csv(core_repos_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts_fields.to_csv('../data/metadata_files/repo_url_cols.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Repo Contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'contributors_url'\n",
    "load_existing_files = True\n",
    "overwrite_existing_temp_files = False\n",
    "join_unique_field = 'repo_full_name'\n",
    "filter_fields = ['repo_full_name', 'login']\n",
    "retry_errors = False\n",
    "contributors_df, users_df = get_repos_user_actors(core_repos, '../data/large_files/join_files/repo_contributors_join_dataset.csv', '../data/large_files/entity_files/users_dataset.csv', get_url_field, load_existing_files, overwrite_existing_temp_files, join_unique_field, filter_fields, retry_errors)\n",
    "contributors_errors_df = check_return_error_file('../data/error_logs/repo_contributors_join_dataset_errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_contributors_df = contributors_df[(contributors_df['repo_full_name'].isin(core_repos['full_name']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120151, 114763)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contributors_df), len(subset_contributors_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 12495 repos, we found 114763 contributors, of which 64856 are unique. There were 6 errors in getting contributors (likely user accounts that no longer exist).\n"
     ]
    }
   ],
   "source": [
    "print(f\"From {len(core_repos)} repos, we found {len(subset_contributors_df)} contributors, of which {len(subset_contributors_df.login.unique())} are unique. There were {len(contributors_errors_df)} errors in getting contributors (likely user accounts that no longer exist).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Repo Starrers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'stargazers_url'\n",
    "load_existing_files = True\n",
    "overwrite_existing_temp_files = False\n",
    "join_unique_field = 'repo_full_name'\n",
    "filter_fields = ['repo_full_name', 'user.login']\n",
    "retry_errors = False\n",
    "stargazers_df, users_df = get_repos_user_actors(core_repos, '../data/large_files/join_files/repo_stargazers_join_dataset.csv', '../data/large_files/entity_files/users_dataset.csv', get_url_field, load_existing_files, overwrite_existing_temp_files, join_unique_field, filter_fields, retry_errors)\n",
    "stargazers_errors_df = check_return_error_file('../data/error_logs/repo_stargazers_join_dataset_errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_stargazers_df = stargazers_df[(stargazers_df['repo_full_name'].isin(core_repos['full_name']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33097, 11263)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stargazers_df), len(subset_stargazers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 12495 repos, we found 11263 stargazers, of which 7792 are unique. There were 2 errors in getting stargazers (likely user accounts that no longer exist).\n"
     ]
    }
   ],
   "source": [
    "print(f\"From {len(core_repos)} repos, we found {len(subset_stargazers_df)} stargazers, of which {len(subset_stargazers_df['user.login'].unique())} are unique. There were {len(stargazers_errors_df)} errors in getting stargazers (likely user accounts that no longer exist).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Repo Subscribers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'subscribers_url'\n",
    "load_existing_files = True\n",
    "overwrite_existing_temp_files = False\n",
    "join_unique_field = 'repo_full_name'\n",
    "filter_fields = ['repo_full_name', 'login']\n",
    "retry_errors = False\n",
    "subscribers_df, users_df = get_repos_user_actors(core_repos, '../data/large_files/join_files/repo_subscribers_join_dataset.csv', '../data/large_files/entity_files/users_dataset.csv', get_url_field, load_existing_files, overwrite_existing_temp_files, join_unique_field, filter_fields, retry_errors)\n",
    "subscribers_errors_df = check_return_error_file('../data/error_logs/repo_subscribers_join_dataset_errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_subscribers_df = subscribers_df[subscribers_df.repo_full_name.isin(core_repos.full_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47047, 25149)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subscribers_df), len(subset_subscribers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 12495 repos, we found 25149 subset_subscribers, of which 2747 are unique. There were 11 errors in getting subscribers (likely user accounts that no longer exist).\n"
     ]
    }
   ],
   "source": [
    "print(f\"From {len(core_repos)} repos, we found {len(subset_subscribers_df)} subset_subscribers, of which {len(subset_subscribers_df.login.unique())} are unique. There were {len(subscribers_errors_df)} errors in getting subscribers (likely user accounts that no longer exist).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cannot get repo collaborators\n",
    "\n",
    "Need Push Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_url_field = 'collaborators_url'\n",
    "# load_existing_data = False\n",
    "# is_stargazers = False\n",
    "# collaborators_df, users_df = get_repos_user_actors(repo_df, '../data/repo_collaborators_join_dataset.csv', '../data/users_dataset.csv', rates_df, get_url_field, load_existing_data, is_stargazers)\n",
    "# collaborators_errors_df = check_return_error_file('../data/error_logs/repo_collaborators_join_dataset_errors.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Repo Forks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'forks_url'\n",
    "load_existing_files = True\n",
    "overwrite_existing_temp_files = False\n",
    "join_unique_field = 'repo_full_name'\n",
    "filter_fields = ['repo_full_name', 'full_name']\n",
    "retry_errors = False\n",
    "forks_df, users_df = get_repos_user_actors(core_repos, '../data/large_files/join_files/repo_forks_join_dataset.csv', '../data/large_files/entity_files/users_dataset.csv', get_url_field, load_existing_files, overwrite_existing_temp_files, join_unique_field, filter_fields, retry_errors)\n",
    "forks_errors_df = check_return_error_file('../data/error_logs/repo_forks_join_dataset_errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_forks_df = forks_df[forks_df.repo_full_name.isin(core_repos.full_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32329, 5594)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(forks_df), len(subset_forks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 12495 repos, we found 5594 subset_forks, of which 4294 are unique. There were 1 errors in getting forks (likely user accounts that no longer exist).\n"
     ]
    }
   ],
   "source": [
    "print(f\"From {len(core_repos)} repos, we found {len(subset_forks_df)} subset_forks, of which {len(subset_forks_df['owner.login'].unique())} are unique. There were {len(forks_errors_df)} errors in getting forks (likely user accounts that no longer exist).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Repo Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'issues_url'\n",
    "load_existing_files = True\n",
    "overwrite_existing_temp_files = False\n",
    "join_unique_field = 'repo_full_name'\n",
    "filter_fields = ['repo_full_name', 'user.login', 'id']\n",
    "retry_errors = False\n",
    "issues_df, users_df = get_repos_user_actors(core_repos, '../data/large_files/join_files/repo_issues_join_dataset.csv', '../data/large_files/entity_files/users_dataset.csv', get_url_field, load_existing_files, overwrite_existing_temp_files, join_unique_field, filter_fields, retry_errors)\n",
    "issues_errors_df = check_return_error_file('../data/error_logs/repo_issues_join_dataset_errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_issues_df = issues_df[issues_df.repo_full_name.isin(core_repos.full_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 12495 repos, we found 41626 issues, which come from 1336 unique repos and were created by 3096 unique users. There were 1 errors in getting issues (likely repos that have no issues longer exist).\n"
     ]
    }
   ],
   "source": [
    "print(f\"From {len(core_repos)} repos, we found {len(subset_issues_df)} issues, which come from {len(subset_issues_df.repository_url.unique())} unique repos and were created by {len(users_df[users_df.login.isin(issues_df['user.login'])])} unique users. There were {len(issues_errors_df)} errors in getting issues (likely repos that have no issues longer exist).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Repo Issue Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'comments_url'\n",
    "load_existing_files = True\n",
    "overwrite_existing_temp_files = False\n",
    "join_unique_field = 'repo_full_name'\n",
    "filter_fields = ['repo_full_name', 'user.login', 'id']\n",
    "retry_errors = False\n",
    "issues_comments_df, users_df = get_repos_user_actors(issues_df, '../data/large_files/join_files/issues_comments_join_dataset.csv', '../data/large_files/entity_files/users_dataset.csv', get_url_field, load_existing_files, overwrite_existing_temp_files, join_unique_field, filter_fields, retry_errors)\n",
    "issues_comments_errors_df = check_return_error_file('../data/error_logs/issues_comments_join_dataset_errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_comments_df = issues_comments_df[issues_comments_df.repo_full_name.isin(core_repos.full_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 90552 repos with issues, we found 36515 comments, which come from 1999 unique repos and were created by 1205 unique users. There were 2505 errors in getting issues comments (likely issues comments longer that no longer exist).\n"
     ]
    }
   ],
   "source": [
    "print(f\"From {len(issues_df)} repos with issues, we found {len(issues_comments_df)} comments, which come from {len(issues_df.repository_url.unique())} unique repos and were created by {len(users_df[users_df.login.isin(issues_comments_df['user.login'])])} unique users. There were {len(issues_comments_errors_df)} errors in getting issues comments (likely issues comments longer that no longer exist).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Repo Pull Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'pulls_url'\n",
    "load_existing_files = True\n",
    "overwrite_existing_temp_files = False\n",
    "filter_fields = ['id', 'repo_full_name', 'user.login', 'head.user.login']\n",
    "join_unique_field = 'repo_full_name'\n",
    "retry_errors = False\n",
    "pulls_df, users_df = get_repos_user_actors(core_repos, '../data/large_files/join_files/repo_pulls_join_dataset.csv', '../data/large_files/entity_files/users_dataset.csv', get_url_field, load_existing_files, overwrite_existing_temp_files, join_unique_field, filter_fields, retry_errors)\n",
    "pulls_errors_df = check_return_error_file('../data/error_logs/repo_pulls_join_dataset_errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pulls_df = pulls_df[pulls_df.repo_full_name.isin(core_repos.full_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 12495 repos, we found 14321 pulls, which come from 599 unique repos and were created by 805 unique users. There were 0 errors in getting pulls (likely repos that have no pulls longer exist).\n"
     ]
    }
   ],
   "source": [
    "print(f\"From {len(core_repos)} repos, we found {len(pulls_df)} pulls, which come from {len(pulls_df.repo_id.unique())} unique repos and were created by {len(users_df[users_df.login.isin(pulls_df['user.login'])])} unique users. There were {len(pulls_errors_df)} errors in getting pulls (likely repos that have no pulls longer exist).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Repo Pull Request Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_type = \"review_comments_url\"\n",
    "count_type = \"review_count\"\n",
    "pulls_df = get_counts(pulls_df, url_type, count_type, overwrite_existing_temp_files=False)\n",
    "pulls_df.to_csv('../data/large_files/join_files/repo_pulls_join_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'review_comments_url'\n",
    "load_existing_files = True\n",
    "overwrite_existing_temp_files = False\n",
    "filter_fields = ['repo_full_name', 'user.login', 'url']\n",
    "join_unique_field = 'repo_full_name'\n",
    "retry_errors = False\n",
    "pulls_comments_df, users_df = get_repos_user_actors(pulls_df, '../data/large_files/join_files/pulls_comments_join_dataset.csv', '../data/large_files/entity_files/users_dataset.csv', get_url_field, load_existing_files, overwrite_existing_temp_files, join_unique_field, filter_fields, retry_errors)\n",
    "pulls_comments_errors_df = check_return_error_file('../data/error_logs/pulls_comments_join_dataset_errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pulls_comments_df = pulls_comments_df[pulls_comments_df.repo_full_name.isin(core_repos.full_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 14321 repos with pulls, we found 6938 comments, which come from 599 unique repos and were created by 189 unique users. There were 16 errors in getting pulls comments (likely repos that have no pulls comments longer exist).\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(f\"From {len(pulls_df)} repos with pulls, we found {len(pulls_comments_df)} comments, which come from {len(pulls_df.repo_id.unique())} unique repos and were created by {len(users_df[users_df.login.isin(pulls_comments_df['user.login'])])} unique users. There were {len(pulls_comments_errors_df)} errors in getting pulls comments (likely repos that have no pulls comments longer exist).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Repo Commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_repos = get_total_commits(core_repos, '../data/large_files/entity_files/subset_repos_dataset_with_commits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 2108 repos, we found 241355 total commits, which considering the 5000 rate limit will take 48.271 hours to get.\n"
     ]
    }
   ],
   "source": [
    "core_repos['cleaned_total_commits'] = core_repos.total_commits.astype(int)\n",
    "print(f\"From {len(core_repos)} repos, we found {core_repos.cleaned_total_commits.sum()} total commits, which considering the 5000 rate limit will take {core_repos.cleaned_total_commits.sum()/5000} hours to get.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'commits_url'\n",
    "load_existing_files = False\n",
    "load_existing_temp_files = True\n",
    "commits_df, users_df = get_repos_user_actors(core_repos, '../data/large_files/join_files/repo_commits_join_dataset.csv', '../data/entity_files/users_dataset.csv', get_url_field, load_existing_files, load_existing_temp_files)\n",
    "commits_errors_df = check_return_error_file('../data/error_logs/repo_commits_join_dataset_errors.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and Explore Repo Specific Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_core_repos = pd.read_csv(\"../data/derived_files/initial_core_repos.csv\")\n",
    "firstpass_core_repos = pd.read_csv(\"../data/derived_files/firstpass_core_repos.csv\")\n",
    "finalpass_core_repos = pd.read_csv(\"../data/derived_files/finalpass_core_repos.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_repos = pd.concat([initial_core_repos, firstpass_core_repos, finalpass_core_repos])\n",
    "core_repos.duplicated(subset=['full_name']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m error_file_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../data/error_logs/repo_profile_errors.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m temp_repo_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../data/temp/repo_profile/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m core_repos \u001b[39m=\u001b[39m get_repo_profile(core_repos, repo_output_path, rates_df, error_file_path, temp_repo_dir)\n",
      "File \u001b[0;32m~/VersionsAndValues/data_cleaning_notebooks/../data_generation_scripts/generate_repo_metadata.py:182\u001b[0m, in \u001b[0;36mget_repo_profile\u001b[0;34m(repo_df, repo_output_path, rates_df, error_file_path, temp_repo_dir)\u001b[0m\n\u001b[1;32m    180\u001b[0m     repos_without_community_profile \u001b[39m=\u001b[39m repo_df\n\u001b[1;32m    181\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mlen\u001b[39m(repos_without_community_profile[repos_without_community_profile\u001b[39m.\u001b[39murl\u001b[39m.\u001b[39mnotna()]) \u001b[39m>\u001b[39m calls_remaining:\n\u001b[0;32m--> 182\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m3700\u001b[39;49m)\n\u001b[1;32m    183\u001b[0m     rates_df \u001b[39m=\u001b[39m check_rate_limit()\n\u001b[1;32m    184\u001b[0m     calls_remaining \u001b[39m=\u001b[39m rates_df[\u001b[39m'\u001b[39m\u001b[39mresources.core.remaining\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "repo_output_path = \"../data/large_files/entity_files/repos_dataset.csv\"\n",
    "error_file_path = \"../data/error_logs/repo_profile_errors.csv\"\n",
    "temp_repo_dir = \"../data/temp/repo_profile/\"\n",
    "core_repos = get_repo_profile(core_repos, repo_output_path, rates_df, error_file_path, temp_repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_repos = get_repo_languages(core_repos, repo_output_path, rates_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_repos = get_repo_labels(core_repos, repo_output_path, rates_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_repos = get_repo_tags(core_repos, repo_output_path, rates_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('values_and_versions_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "92b14f494f00d59c3bde62efd18715990e48408449b0a7aa8b9d827d8bcdd450"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
