{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process DH Repos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will be run multiple times as we add new repositories to the DH repo list. It will take the data from the DH repo list and process it through making additional calls to GitHub's APIs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries and Set Initial Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv(\"../../datasets/derived_files/grouped_cleaned_translated_dh_terms.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = pd.read_csv(\"../../datasets/derived_files/dh_terms_with_multiple_codes.csv\")\n",
    "cleaned_dh = pd.read_csv(\"../../datasets/derived_files/cleaned_translated_dh_terms.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "directionality_df = pd.read_csv(\"../../datasets/metadata_files/iso_639_choices_directionality_wikimedia.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset directionality to LTR and RTL languages\n",
    "directionality_df = directionality_df[directionality_df.directionality.isin(['ltr', 'rtl'])]\n",
    "\n",
    "# Merge the directionality data with the cleaned terms\n",
    "merged_lang_terms = pd.merge(directionality_df[['code', 'directionality', 'English language name', 'local language name']], cleaned_dh, on='code', how=\"outer\")\n",
    "merged_lang_terms = merged_lang_terms[merged_lang_terms.code != \"see also Test languages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_dh = merged_lang_terms[merged_lang_terms.term_source == 'Digital Humanities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_source</th>\n",
       "      <th>term</th>\n",
       "      <th>directionality</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Digital Humanities</td>\n",
       "      <td>Digital Humanities</td>\n",
       "      <td>rtl</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          term_source                term directionality  counts\n",
       "9  Digital Humanities  Digital Humanities            rtl       1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing = subset_dh.groupby(['term_source', 'term', 'directionality']).size().reset_index(name='counts').sort_values(by='counts', ascending=False)\n",
    "\n",
    "testing[testing.term.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_source</th>\n",
       "      <th>term</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>Digital Humanities</td>\n",
       "      <td>Digital Humanities</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>Digital Humanities</td>\n",
       "      <td>Humanidades Dixitais</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>Digital Humanities</td>\n",
       "      <td>Humaniora Digital</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>Digital Humanities</td>\n",
       "      <td>Humanistyka cyfrowa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>Digital Humanities</td>\n",
       "      <td>Humanitate Digitalak</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>Cultural Analytics</td>\n",
       "      <td>مەدەنىيەت ئانالىزى</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>Cultural Analytics</td>\n",
       "      <td>کلتوري تحلیلونه</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>Cultural Analytics</td>\n",
       "      <td>ކަލްޗަރަލް އެނަލިޓިކްސް</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>Cultural Analytics</td>\n",
       "      <td>सांस्कृतिक विश्लेषण</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>Public History</td>\n",
       "      <td>공공 역사</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>858 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            term_source                     term  counts\n",
       "537  Digital Humanities       Digital Humanities       2\n",
       "563  Digital Humanities     Humanidades Dixitais       1\n",
       "565  Digital Humanities        Humaniora Digital       1\n",
       "566  Digital Humanities      Humanistyka cyfrowa       1\n",
       "567  Digital Humanities     Humanitate Digitalak       1\n",
       "..                  ...                      ...     ...\n",
       "289  Cultural Analytics       مەدەنىيەت ئانالىزى       1\n",
       "290  Cultural Analytics          کلتوري تحلیلونه       1\n",
       "291  Cultural Analytics  ކަލްޗަރަލް އެނަލިޓިކްސް       1\n",
       "292  Cultural Analytics      सांस्कृतिक विश्लेषण       1\n",
       "857      Public History                    공공 역사       1\n",
       "\n",
       "[858 rows x 3 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_lang_terms.groupby(['term_source','term']).directionality.nunique().reset_index(name='counts').sort_values(by='counts', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dh =merged_lang_terms.groupby(['term_source','term', 'directionality']).agg({'code': ','.join, 'term': 'count', 'English language name': ', '.join }).reset_index(level=0)\n",
    "grouped_dh['processing'] = grouped_dh.index\n",
    "# Split the 'processing' column into two columns\n",
    "grouped_dh[['final_term', 'directionality']] = grouped_dh['processing'].apply(pd.Series)\n",
    "\n",
    "# Now you can drop the 'processing' column if you want\n",
    "grouped_dh = grouped_dh.drop('processing', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dh = grouped_dh.reset_index(drop=True)\n",
    "grouped_dh = grouped_dh.rename(columns={'term': 'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_source</th>\n",
       "      <th>code</th>\n",
       "      <th>count</th>\n",
       "      <th>English language name</th>\n",
       "      <th>final_term</th>\n",
       "      <th>directionality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Computational Humanities</td>\n",
       "      <td>bs,da,de,en,fy,mg,no,ny,sm,sn,st,sv,tl,nb</td>\n",
       "      <td>14</td>\n",
       "      <td>Bosnian, Danish, German, English, West Frisian...</td>\n",
       "      <td>Computational Humanities</td>\n",
       "      <td>ltr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Computational Social Science</td>\n",
       "      <td>da,en,la,mg,mi,ny,sm,sn,st,tl</td>\n",
       "      <td>10</td>\n",
       "      <td>Danish, English, Latin, Malagasy, Māori, Chich...</td>\n",
       "      <td>Computational Social Science</td>\n",
       "      <td>ltr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>Digital Humanities</td>\n",
       "      <td>en,ig,lb,mg,ny,sm,sn,st,tl,yo</td>\n",
       "      <td>10</td>\n",
       "      <td>English, Igbo, Luxembourgish, Malagasy, Chiche...</td>\n",
       "      <td>Digital Humanities</td>\n",
       "      <td>ltr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>Cultural Analytics</td>\n",
       "      <td>bs,en,ky,lb,mg,ny,sn,tl</td>\n",
       "      <td>8</td>\n",
       "      <td>Bosnian, English, Kirghiz, Luxembourgish, Mala...</td>\n",
       "      <td>Cultural Analytics</td>\n",
       "      <td>ltr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>Digital History</td>\n",
       "      <td>bs,en,fy,ky,la,sn,yo</td>\n",
       "      <td>7</td>\n",
       "      <td>Bosnian, English, West Frisian, Kirghiz, Latin...</td>\n",
       "      <td>Digital History</td>\n",
       "      <td>ltr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>Cultural Analytics</td>\n",
       "      <td>si</td>\n",
       "      <td>1</td>\n",
       "      <td>Sinhalese</td>\n",
       "      <td>සංස්කෘතික විශ්ලේෂණ</td>\n",
       "      <td>ltr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>Cultural Analytics</td>\n",
       "      <td>th</td>\n",
       "      <td>1</td>\n",
       "      <td>Thai</td>\n",
       "      <td>การวิเคราะห์ทางวัฒนธรรม</td>\n",
       "      <td>ltr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>Cultural Analytics</td>\n",
       "      <td>lo</td>\n",
       "      <td>1</td>\n",
       "      <td>Laotian</td>\n",
       "      <td>ການວິເຄາະວັດທະນະທໍາ</td>\n",
       "      <td>ltr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>Cultural Analytics</td>\n",
       "      <td>my</td>\n",
       "      <td>1</td>\n",
       "      <td>Burmese</td>\n",
       "      <td>ယဉ်ကျေးမှုပိုင်းခြားစိတ်ဖြာမှု</td>\n",
       "      <td>ltr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>Public History</td>\n",
       "      <td>ko</td>\n",
       "      <td>1</td>\n",
       "      <td>Korean</td>\n",
       "      <td>공공 역사</td>\n",
       "      <td>ltr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>787 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      term_source                                       code  \\\n",
       "4        Computational Humanities  bs,da,de,en,fy,mg,no,ny,sm,sn,st,sv,tl,nb   \n",
       "115  Computational Social Science              da,en,la,mg,mi,ny,sm,sn,st,tl   \n",
       "537            Digital Humanities              en,ig,lb,mg,ny,sm,sn,st,tl,yo   \n",
       "233            Cultural Analytics                    bs,en,ky,lb,mg,ny,sn,tl   \n",
       "428               Digital History                       bs,en,fy,ky,la,sn,yo   \n",
       "..                            ...                                        ...   \n",
       "302            Cultural Analytics                                         si   \n",
       "303            Cultural Analytics                                         th   \n",
       "304            Cultural Analytics                                         lo   \n",
       "305            Cultural Analytics                                         my   \n",
       "858                Public History                                         ko   \n",
       "\n",
       "     count                              English language name  \\\n",
       "4       14  Bosnian, Danish, German, English, West Frisian...   \n",
       "115     10  Danish, English, Latin, Malagasy, Māori, Chich...   \n",
       "537     10  English, Igbo, Luxembourgish, Malagasy, Chiche...   \n",
       "233      8  Bosnian, English, Kirghiz, Luxembourgish, Mala...   \n",
       "428      7  Bosnian, English, West Frisian, Kirghiz, Latin...   \n",
       "..     ...                                                ...   \n",
       "302      1                                          Sinhalese   \n",
       "303      1                                               Thai   \n",
       "304      1                                            Laotian   \n",
       "305      1                                            Burmese   \n",
       "858      1                                             Korean   \n",
       "\n",
       "                         final_term directionality  \n",
       "4          Computational Humanities            ltr  \n",
       "115    Computational Social Science            ltr  \n",
       "537              Digital Humanities            ltr  \n",
       "233              Cultural Analytics            ltr  \n",
       "428                 Digital History            ltr  \n",
       "..                              ...            ...  \n",
       "302              සංස්කෘතික විශ්ලේෂණ            ltr  \n",
       "303         การวิเคราะห์ทางวัฒนธรรม            ltr  \n",
       "304             ການວິເຄາະວັດທະນະທໍາ            ltr  \n",
       "305  ယဉ်ကျေးမှုပိုင်းခြားစိတ်ဖြာမှု            ltr  \n",
       "858                           공공 역사            ltr  \n",
       "\n",
       "[787 rows x 6 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_dh[grouped_dh.directionality == 'ltr'].sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from data_generation_scripts.utils import check_rate_limit, check_return_error_file, read_combine_files, check_total_pages, check_for_joins_in_older_queries, check_add_users, combined_updated_users, check_for_entity_in_older_queries\n",
    "from data_generation_scripts.generate_expanded_search_data import get_initial_search_datasets\n",
    "from data_generation_scripts.generate_repo_metadata import get_repo_languages, get_repo_labels, get_repo_tags,  get_repo_profile, get_total_commits\n",
    "from data_generation_scripts.generate_repo_users_interactions import get_repos_user_actors\n",
    "from data_generation_scripts.generate_repo_metadata import check_total_results\n",
    "from data_generation_scripts.general_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates_df = check_rate_limit()\n",
    "datafile_path = \"../../datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_core_repos = read_csv_file(f\"{data_directory_path}/derived_files/initial_core_repos.csv\")\n",
    "core_repos = read_csv_file(f\"{data_directory_path}/derived_files/firstpass_core_repos.csv\")\n",
    "len(initial_core_repos), len(core_repos)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Missing Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_fields = read_csv_file(f'{data_directory_path}/metadata_files/repo_url_cols.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_fields.loc[counts_fields.url_type == 'review_comments_url', 'count_type'] = 'review_count'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(repo_df, url_type, count_type, overwrite_existing_temp_files = False):\n",
    "    if count_type in repo_df.columns:\n",
    "        needs_counts = repo_df[repo_df[count_type].isna()]\n",
    "        has_counts = repo_df[repo_df[count_type].notna()]\n",
    "    else:\n",
    "        needs_counts = repo_df\n",
    "        has_counts = pd.DataFrame()\n",
    "        \n",
    "    if len(has_counts) == len(repo_df):\n",
    "        repo_df = has_counts\n",
    "    else:\n",
    "        needs_counts = check_total_results(needs_counts, url_type, overwrite_existing_temp_files)\n",
    "        repo_df = pd.concat([needs_counts, has_counts])\n",
    "    return repo_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting pulls_count for pulls_url\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "skip_types = ['review_comments_url', 'commits_url', 'collaborators_url']\n",
    "overwrite_existing_temp_files = True\n",
    "for index, row in counts_fields.iterrows():\n",
    "    if (row.url_type not in skip_types):\n",
    "        count_type = row.url_type.split(\"_\")[0] + \"_count\"\n",
    "        print(f\"Getting {count_type} for {row['url_type']}\")\n",
    "        if (count_type not in core_repos.columns) or (core_repos[count_type].isna().any()):\n",
    "            core_repos = get_counts(core_repos, row['url_type'], count_type, overwrite_existing_temp_files)\n",
    "        row['count_type'] = count_type\n",
    "        clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core_repos.to_csv(core_repos_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts_fields.to_csv('../data/metadata_files/repo_url_cols.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Repo Contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'contributors_url'\n",
    "load_existing_files = True\n",
    "overwrite_existing_temp_files = False\n",
    "join_unique_field = 'repo_full_name'\n",
    "filter_fields = ['repo_full_name', 'login']\n",
    "retry_errors = False\n",
    "contributors_df, users_df = get_repos_user_actors(core_repos, '../data/large_files/join_files/repo_contributors_join_dataset.csv', '../data/large_files/entity_files/users_dataset.csv', get_url_field, load_existing_files, overwrite_existing_temp_files, join_unique_field, filter_fields, retry_errors)\n",
    "contributors_errors_df = check_return_error_file('../data/error_logs/repo_contributors_join_dataset_errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_contributors_df = contributors_df[(contributors_df['repo_full_name'].isin(core_repos['full_name']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120151, 114763)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contributors_df), len(subset_contributors_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 12495 repos, we found 114763 contributors, of which 64856 are unique. There were 6 errors in getting contributors (likely user accounts that no longer exist).\n"
     ]
    }
   ],
   "source": [
    "print(f\"From {len(core_repos)} repos, we found {len(subset_contributors_df)} contributors, of which {len(subset_contributors_df.login.unique())} are unique. There were {len(contributors_errors_df)} errors in getting contributors (likely user accounts that no longer exist).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Repo Starrers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'stargazers_url'\n",
    "load_existing_files = True\n",
    "overwrite_existing_temp_files = False\n",
    "join_unique_field = 'repo_full_name'\n",
    "filter_fields = ['repo_full_name', 'user.login']\n",
    "retry_errors = False\n",
    "stargazers_df, users_df = get_repos_user_actors(core_repos, '../data/large_files/join_files/repo_stargazers_join_dataset.csv', '../data/large_files/entity_files/users_dataset.csv', get_url_field, load_existing_files, overwrite_existing_temp_files, join_unique_field, filter_fields, retry_errors)\n",
    "stargazers_errors_df = check_return_error_file('../data/error_logs/repo_stargazers_join_dataset_errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_stargazers_df = stargazers_df[(stargazers_df['repo_full_name'].isin(core_repos['full_name']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33097, 11263)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stargazers_df), len(subset_stargazers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 12495 repos, we found 11263 stargazers, of which 7792 are unique. There were 2 errors in getting stargazers (likely user accounts that no longer exist).\n"
     ]
    }
   ],
   "source": [
    "print(f\"From {len(core_repos)} repos, we found {len(subset_stargazers_df)} stargazers, of which {len(subset_stargazers_df['user.login'].unique())} are unique. There were {len(stargazers_errors_df)} errors in getting stargazers (likely user accounts that no longer exist).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Repo Subscribers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'subscribers_url'\n",
    "load_existing_files = True\n",
    "overwrite_existing_temp_files = False\n",
    "join_unique_field = 'repo_full_name'\n",
    "filter_fields = ['repo_full_name', 'login']\n",
    "retry_errors = False\n",
    "subscribers_df, users_df = get_repos_user_actors(core_repos, '../data/large_files/join_files/repo_subscribers_join_dataset.csv', '../data/large_files/entity_files/users_dataset.csv', get_url_field, load_existing_files, overwrite_existing_temp_files, join_unique_field, filter_fields, retry_errors)\n",
    "subscribers_errors_df = check_return_error_file('../data/error_logs/repo_subscribers_join_dataset_errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_subscribers_df = subscribers_df[subscribers_df.repo_full_name.isin(core_repos.full_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47047, 25149)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subscribers_df), len(subset_subscribers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 12495 repos, we found 25149 subset_subscribers, of which 2747 are unique. There were 11 errors in getting subscribers (likely user accounts that no longer exist).\n"
     ]
    }
   ],
   "source": [
    "print(f\"From {len(core_repos)} repos, we found {len(subset_subscribers_df)} subset_subscribers, of which {len(subset_subscribers_df.login.unique())} are unique. There were {len(subscribers_errors_df)} errors in getting subscribers (likely user accounts that no longer exist).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cannot get repo collaborators\n",
    "\n",
    "Need Push Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_url_field = 'collaborators_url'\n",
    "# load_existing_data = False\n",
    "# is_stargazers = False\n",
    "# collaborators_df, users_df = get_repos_user_actors(repo_df, '../data/repo_collaborators_join_dataset.csv', '../data/users_dataset.csv', rates_df, get_url_field, load_existing_data, is_stargazers)\n",
    "# collaborators_errors_df = check_return_error_file('../data/error_logs/repo_collaborators_join_dataset_errors.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Repo Forks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'forks_url'\n",
    "load_existing_files = True\n",
    "overwrite_existing_temp_files = False\n",
    "join_unique_field = 'repo_full_name'\n",
    "filter_fields = ['repo_full_name', 'full_name']\n",
    "retry_errors = False\n",
    "forks_df, users_df = get_repos_user_actors(core_repos, '../data/large_files/join_files/repo_forks_join_dataset.csv', '../data/large_files/entity_files/users_dataset.csv', get_url_field, load_existing_files, overwrite_existing_temp_files, join_unique_field, filter_fields, retry_errors)\n",
    "forks_errors_df = check_return_error_file('../data/error_logs/repo_forks_join_dataset_errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_forks_df = forks_df[forks_df.repo_full_name.isin(core_repos.full_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32329, 5594)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(forks_df), len(subset_forks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 12495 repos, we found 5594 subset_forks, of which 4294 are unique. There were 1 errors in getting forks (likely user accounts that no longer exist).\n"
     ]
    }
   ],
   "source": [
    "print(f\"From {len(core_repos)} repos, we found {len(subset_forks_df)} subset_forks, of which {len(subset_forks_df['owner.login'].unique())} are unique. There were {len(forks_errors_df)} errors in getting forks (likely user accounts that no longer exist).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Repo Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'issues_url'\n",
    "load_existing_files = True\n",
    "overwrite_existing_temp_files = False\n",
    "join_unique_field = 'repo_full_name'\n",
    "filter_fields = ['repo_full_name', 'user.login', 'id']\n",
    "retry_errors = False\n",
    "issues_df, users_df = get_repos_user_actors(core_repos, '../data/large_files/join_files/repo_issues_join_dataset.csv', '../data/large_files/entity_files/users_dataset.csv', get_url_field, load_existing_files, overwrite_existing_temp_files, join_unique_field, filter_fields, retry_errors)\n",
    "issues_errors_df = check_return_error_file('../data/error_logs/repo_issues_join_dataset_errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_issues_df = issues_df[issues_df.repo_full_name.isin(core_repos.full_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 12495 repos, we found 41626 issues, which come from 1336 unique repos and were created by 3096 unique users. There were 1 errors in getting issues (likely repos that have no issues longer exist).\n"
     ]
    }
   ],
   "source": [
    "print(f\"From {len(core_repos)} repos, we found {len(subset_issues_df)} issues, which come from {len(subset_issues_df.repository_url.unique())} unique repos and were created by {len(users_df[users_df.login.isin(issues_df['user.login'])])} unique users. There were {len(issues_errors_df)} errors in getting issues (likely repos that have no issues longer exist).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Repo Issue Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'comments_url'\n",
    "load_existing_files = True\n",
    "overwrite_existing_temp_files = False\n",
    "join_unique_field = 'repo_full_name'\n",
    "filter_fields = ['repo_full_name', 'user.login', 'id']\n",
    "retry_errors = False\n",
    "issues_comments_df, users_df = get_repos_user_actors(issues_df, '../data/large_files/join_files/issues_comments_join_dataset.csv', '../data/large_files/entity_files/users_dataset.csv', get_url_field, load_existing_files, overwrite_existing_temp_files, join_unique_field, filter_fields, retry_errors)\n",
    "issues_comments_errors_df = check_return_error_file('../data/error_logs/issues_comments_join_dataset_errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_comments_df = issues_comments_df[issues_comments_df.repo_full_name.isin(core_repos.full_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 90552 repos with issues, we found 36515 comments, which come from 1999 unique repos and were created by 1205 unique users. There were 2505 errors in getting issues comments (likely issues comments longer that no longer exist).\n"
     ]
    }
   ],
   "source": [
    "print(f\"From {len(issues_df)} repos with issues, we found {len(issues_comments_df)} comments, which come from {len(issues_df.repository_url.unique())} unique repos and were created by {len(users_df[users_df.login.isin(issues_comments_df['user.login'])])} unique users. There were {len(issues_comments_errors_df)} errors in getting issues comments (likely issues comments longer that no longer exist).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Repo Pull Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'pulls_url'\n",
    "load_existing_files = True\n",
    "overwrite_existing_temp_files = False\n",
    "filter_fields = ['id', 'repo_full_name', 'user.login', 'head.user.login']\n",
    "join_unique_field = 'repo_full_name'\n",
    "retry_errors = False\n",
    "pulls_df, users_df = get_repos_user_actors(core_repos, '../data/large_files/join_files/repo_pulls_join_dataset.csv', '../data/large_files/entity_files/users_dataset.csv', get_url_field, load_existing_files, overwrite_existing_temp_files, join_unique_field, filter_fields, retry_errors)\n",
    "pulls_errors_df = check_return_error_file('../data/error_logs/repo_pulls_join_dataset_errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pulls_df = pulls_df[pulls_df.repo_full_name.isin(core_repos.full_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 12495 repos, we found 14321 pulls, which come from 599 unique repos and were created by 805 unique users. There were 0 errors in getting pulls (likely repos that have no pulls longer exist).\n"
     ]
    }
   ],
   "source": [
    "print(f\"From {len(core_repos)} repos, we found {len(pulls_df)} pulls, which come from {len(pulls_df.repo_id.unique())} unique repos and were created by {len(users_df[users_df.login.isin(pulls_df['user.login'])])} unique users. There were {len(pulls_errors_df)} errors in getting pulls (likely repos that have no pulls longer exist).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Repo Pull Request Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_type = \"review_comments_url\"\n",
    "count_type = \"review_count\"\n",
    "pulls_df = get_counts(pulls_df, url_type, count_type, overwrite_existing_temp_files=False)\n",
    "pulls_df.to_csv('../data/large_files/join_files/repo_pulls_join_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'review_comments_url'\n",
    "load_existing_files = True\n",
    "overwrite_existing_temp_files = False\n",
    "filter_fields = ['repo_full_name', 'user.login', 'url']\n",
    "join_unique_field = 'repo_full_name'\n",
    "retry_errors = False\n",
    "pulls_comments_df, users_df = get_repos_user_actors(pulls_df, '../data/large_files/join_files/pulls_comments_join_dataset.csv', '../data/large_files/entity_files/users_dataset.csv', get_url_field, load_existing_files, overwrite_existing_temp_files, join_unique_field, filter_fields, retry_errors)\n",
    "pulls_comments_errors_df = check_return_error_file('../data/error_logs/pulls_comments_join_dataset_errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pulls_comments_df = pulls_comments_df[pulls_comments_df.repo_full_name.isin(core_repos.full_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 14321 repos with pulls, we found 6938 comments, which come from 599 unique repos and were created by 189 unique users. There were 16 errors in getting pulls comments (likely repos that have no pulls comments longer exist).\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(f\"From {len(pulls_df)} repos with pulls, we found {len(pulls_comments_df)} comments, which come from {len(pulls_df.repo_id.unique())} unique repos and were created by {len(users_df[users_df.login.isin(pulls_comments_df['user.login'])])} unique users. There were {len(pulls_comments_errors_df)} errors in getting pulls comments (likely repos that have no pulls comments longer exist).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Repo Commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_repos = get_total_commits(core_repos, '../data/large_files/entity_files/subset_repos_dataset_with_commits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 2108 repos, we found 241355 total commits, which considering the 5000 rate limit will take 48.271 hours to get.\n"
     ]
    }
   ],
   "source": [
    "core_repos['cleaned_total_commits'] = core_repos.total_commits.astype(int)\n",
    "print(f\"From {len(core_repos)} repos, we found {core_repos.cleaned_total_commits.sum()} total commits, which considering the 5000 rate limit will take {core_repos.cleaned_total_commits.sum()/5000} hours to get.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'commits_url'\n",
    "load_existing_files = False\n",
    "load_existing_temp_files = True\n",
    "commits_df, users_df = get_repos_user_actors(core_repos, '../data/large_files/join_files/repo_commits_join_dataset.csv', '../data/entity_files/users_dataset.csv', get_url_field, load_existing_files, load_existing_temp_files)\n",
    "commits_errors_df = check_return_error_file('../data/error_logs/repo_commits_join_dataset_errors.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and Explore Repo Specific Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_core_repos = pd.read_csv(\"../data/derived_files/initial_core_repos.csv\")\n",
    "firstpass_core_repos = pd.read_csv(\"../data/derived_files/firstpass_core_repos.csv\")\n",
    "finalpass_core_repos = pd.read_csv(\"../data/derived_files/finalpass_core_repos.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_repos = pd.concat([initial_core_repos, firstpass_core_repos, finalpass_core_repos])\n",
    "core_repos.duplicated(subset=['full_name']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m error_file_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../data/error_logs/repo_profile_errors.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m temp_repo_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../data/temp/repo_profile/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m core_repos \u001b[39m=\u001b[39m get_repo_profile(core_repos, repo_output_path, rates_df, error_file_path, temp_repo_dir)\n",
      "File \u001b[0;32m~/VersionsAndValues/data_cleaning_notebooks/../data_generation_scripts/generate_repo_metadata.py:182\u001b[0m, in \u001b[0;36mget_repo_profile\u001b[0;34m(repo_df, repo_output_path, rates_df, error_file_path, temp_repo_dir)\u001b[0m\n\u001b[1;32m    180\u001b[0m     repos_without_community_profile \u001b[39m=\u001b[39m repo_df\n\u001b[1;32m    181\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mlen\u001b[39m(repos_without_community_profile[repos_without_community_profile\u001b[39m.\u001b[39murl\u001b[39m.\u001b[39mnotna()]) \u001b[39m>\u001b[39m calls_remaining:\n\u001b[0;32m--> 182\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m3700\u001b[39;49m)\n\u001b[1;32m    183\u001b[0m     rates_df \u001b[39m=\u001b[39m check_rate_limit()\n\u001b[1;32m    184\u001b[0m     calls_remaining \u001b[39m=\u001b[39m rates_df[\u001b[39m'\u001b[39m\u001b[39mresources.core.remaining\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "repo_output_path = \"../data/large_files/entity_files/repos_dataset.csv\"\n",
    "error_file_path = \"../data/error_logs/repo_profile_errors.csv\"\n",
    "temp_repo_dir = \"../data/temp/repo_profile/\"\n",
    "core_repos = get_repo_profile(core_repos, repo_output_path, rates_df, error_file_path, temp_repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_repos = get_repo_languages(core_repos, repo_output_path, rates_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_repos = get_repo_labels(core_repos, repo_output_path, rates_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_repos = get_repo_tags(core_repos, repo_output_path, rates_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('values_and_versions_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "92b14f494f00d59c3bde62efd18715990e48408449b0a7aa8b9d827d8bcdd450"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
